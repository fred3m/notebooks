{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of slicing and gradient updates\n",
    "\n",
    "For our project there are two primary places that we need to optimize, inserting a large number of small patches into a larger images and convolutions. This notebook is to demonstrate that just inserting the models using JAX is significantly more expensive than autograd (and directly doing the gradients with numpy) to the point where it is unusable for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import autograd.numpy as anp\n",
    "import autograd\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct gradients\n",
    "\n",
    "We first illustrate the base case with direct gradients. Our actual models require FFTs and convolutions, but this represents the core of our algorithm that appears difficult to reproduce in JAX without significant performance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_model(sed, morph, center):\n",
    "    \"\"\"Get a model for a single component\n",
    "    \"\"\"\n",
    "    y,x = center\n",
    "    box = (slice(y-1, y+2), slice(x-1, x+2))\n",
    "    return sed[:, None, None]*morph[None, :, :], box\n",
    "\n",
    "def grad_get_component_model(chain_grad, model, sed, morph, center):\n",
    "    \"\"\"Calculate the gradient for a single component model\n",
    "    \"\"\"\n",
    "    y,x = center\n",
    "    box = (slice(y-1, y+2), slice(x-1, x+2))\n",
    "    _chain_grad = chain_grad[:, box[0], box[1]]\n",
    "    sed_grad = np.einsum(\"...jk,jk\", _chain_grad, morph)\n",
    "    morph_grad = np.einsum(\"i,i...\", sed, _chain_grad)\n",
    "    return sed_grad, morph_grad\n",
    "\n",
    "def get_full_model(seds, morphs, centers, shape):\n",
    "    \"\"\"Combine all of the single component models into one model\n",
    "    \"\"\"\n",
    "    model = np.zeros(shape)\n",
    "    boxes = []\n",
    "    for k in range(len(seds)):\n",
    "        _model, box = get_component_model(seds[k], morphs[k], centers[k])\n",
    "        model[:, box[0], box[1]] += _model\n",
    "        boxes.append(box)\n",
    "    return model, box\n",
    "\n",
    "def grad_get_full_model(chain_grad, model, centers, seds, morphs):\n",
    "    \"\"\"Collect the gradients of all the single component models\n",
    "    \n",
    "    This would also multiply by dM/dM_i,\n",
    "    except that all models have equal weights so `dM/dM_i=1`.\n",
    "    \"\"\"\n",
    "    sed_grads = []\n",
    "    morph_grads = []\n",
    "    for k in range(len(centers)):\n",
    "        grads = grad_get_component_model(chain_grad, model, seds[k], morphs[k], centers[k])\n",
    "        sed_grads.append(grads[0])\n",
    "        morph_grads.append(grads[1])\n",
    "    return np.array(sed_grads), np.array(morph_grads)\n",
    "\n",
    "def logL(data, model, weights=1):\n",
    "    \"\"\"Log-likelihood\n",
    "    \"\"\"\n",
    "    return 0.5*weights*(data-model)**2\n",
    "\n",
    "def grad_logL(data, model, weights=1):\n",
    "    \"\"\"Gradient of the log-likelihood\n",
    "    \"\"\"\n",
    "    return weights*(model-data)\n",
    "\n",
    "def calculate_gradients_direct(images, centers, seds, morphs):\n",
    "    model, boxes = get_full_model(seds, morphs, centers, images.shape)\n",
    "    gL = grad_logL(images, model)\n",
    "    g_direct = grad_get_full_model(gL, model, centers, seds, morphs)\n",
    "    return g_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd gradients\n",
    "\n",
    "We reuse `get_component_model` and `logL` but rewrite the other methods to build our models and take their gradients with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "@autograd.extend.primitive\n",
    "def add_slice(destination, source, slices):\n",
    "    destination[slices] += source\n",
    "    return destination\n",
    "\n",
    "def grad_add_slice_dest(result, destination, source, slices):\n",
    "    return lambda grad_chain: grad_chain\n",
    "\n",
    "def grad_add_slice_src(result, destination, source, slices):\n",
    "    def result(grad_chain):\n",
    "        return grad_chain[slices]\n",
    "    return result\n",
    "\n",
    "# Register this function in autograd\n",
    "autograd.extend.defvjp(add_slice, grad_add_slice_dest, grad_add_slice_src)\n",
    "\n",
    "def get_full_model_autograd(seds, morphs, centers, shape):\n",
    "    model = anp.zeros(shape)\n",
    "    boxes = []\n",
    "    for k in range(len(seds)):\n",
    "        _model, box = get_component_model(seds[k], morphs[k], centers[k])\n",
    "        box = (slice(None),) + box\n",
    "        model = add_slice(model, _model, box)\n",
    "        boxes.append(box)\n",
    "    return model, box\n",
    "\n",
    "def loss_autograd(seds, morphs, image):\n",
    "    \"\"\"This is the method that autograd uses to calculate the gradients\n",
    "    \"\"\"\n",
    "    model, boxes = get_full_model_autograd(seds, morphs, centers, image.shape)\n",
    "    return anp.sum(logL(image, model))\n",
    "\n",
    "def calculate_gradients_autograd(images, centers, seds, morphs):\n",
    "    # Initialize the autograd loss function\n",
    "    _grad = autograd.grad(partial(loss_autograd, image=images), tuple(range(2)))\n",
    "    g_auto = _grad(seds, morphs)\n",
    "    \n",
    "    return g_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX gradients\n",
    "\n",
    "This code produces the same results while using JAX and `jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model_jax(seds, morphs, centers, shape):\n",
    "    model = jnp.zeros(shape)\n",
    "    boxes = []\n",
    "    for k in range(len(seds)):\n",
    "        _model, box = get_component_model(seds[k], morphs[k], centers[k])\n",
    "        box = (slice(None),) + box\n",
    "        model = jax.ops.index_add(model, jax.ops.index[box], _model)\n",
    "        boxes.append(box)\n",
    "    return model, box\n",
    "\n",
    "def loss_jax(seds, morphs, image, centers):\n",
    "    \"\"\"This is the method that jax uses to calculate the gradients\n",
    "    \"\"\"\n",
    "    model, boxes = get_full_model_jax(seds, morphs, centers, image.shape)\n",
    "    return jnp.sum(logL(image, model))\n",
    "\n",
    "def calculate_gradients_jax(images, centers, seds, morphs):\n",
    "    _grad = jax.grad(partial(loss_jax, image=images, centers=centers), tuple(range(2)))\n",
    "    g_jax = _grad(seds, morphs)\n",
    "    \n",
    "    return g_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIT gradients\n",
    "\n",
    "This is the same code as in the JAX section, wrapped in jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_component_model_jit(sed, morph):\n",
    "    \"\"\"Get a model for a single component\n",
    "    \"\"\"\n",
    "    shape = (sed.shape[0], morph.shape[0], morph.shape[1])\n",
    "    _sed = jax.lax.broadcast_in_dim(sed, shape, (0,))\n",
    "    _morph = jax.lax.broadcast_in_dim(morph, shape, (1,2))\n",
    "    return _sed * _morph\n",
    "\n",
    "def _update_model(model, k, seds, morphs, centers):\n",
    "    y,x = centers[k]\n",
    "    _model = get_component_model_jit(seds[k], morphs[k])\n",
    "    start_indices = (0, y-1, x-1)\n",
    "    _model += jax.lax.dynamic_slice(model, start_indices, _model.shape)\n",
    "    model = jax.lax.dynamic_update_slice(model, _model, start_indices)\n",
    "    return model, None\n",
    "\n",
    "def get_full_model_jit(seds, morphs, centers, shape):\n",
    "    model = jnp.zeros(shape)\n",
    "    boxes = []\n",
    "    func = partial(_update_model, seds=seds, morphs=morphs, centers=centers)\n",
    "    model, _ = jax.lax.scan(func, model, np.arange(len(seds)))\n",
    "    return model\n",
    "\n",
    "def loss_jit(seds, morphs, image, centers):\n",
    "    \"\"\"This is the method that jax uses to calculate the gradients\n",
    "    \"\"\"\n",
    "    model = get_full_model_jit(seds, morphs, centers, image.shape)\n",
    "    return jnp.sum(logL(image, model))\n",
    "\n",
    "def calculate_gradients_jit(images, centers, seds, morphs):\n",
    "    _grad = jax.grad(partial(loss_jit, image=images, centers=centers), tuple(range(2)))\n",
    "    g_jax = _grad(seds, morphs)\n",
    "    \n",
    "    return g_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Display the model\n",
    "\n",
    "This builds the model and allows us to display it as an RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2rgb(image):\n",
    "    \"\"\"Display an image as an array\n",
    "    \"\"\"\n",
    "    rgb = image.copy()\n",
    "    rgb[rgb>255] = 255\n",
    "    rgb[rgb<0] = 0\n",
    "    rgb = rgb.astype(np.uint8)\n",
    "    rgb = np.transpose(rgb, axes=(1,2,0))\n",
    "    return rgb\n",
    "\n",
    "def build_scene(shape, K, figsize=None, show=True):\n",
    "    \"\"\"Build an image and models for a collection of sources\n",
    "    \"\"\"\n",
    "    B, N, M = shape\n",
    "    images = np.zeros(shape)\n",
    "    centers = np.zeros((K, 2), dtype=int)\n",
    "    seds = np.zeros((K, B))\n",
    "    morphs = np.zeros((K, 3, 3))\n",
    "    morphs[:, 1, 1] = 1\n",
    "    \n",
    "    for k in range(K):\n",
    "        morphs[k] = np.arange(9).reshape(3, 3)\n",
    "\n",
    "    for k in range(K):\n",
    "        cy = np.random.randint(4, images.shape[1]-4)\n",
    "        cx = np.random.randint(4, images.shape[2]-4)\n",
    "        center = np.array([cy, cx])\n",
    "        sed = np.random.rand(B)\n",
    "        sed *= 500/np.sum(sed)\n",
    "        centers[k] = center\n",
    "        y,x = center\n",
    "        images[:, y-1:y+2, x-1:x+2] += morphs[k][None, :, :] * sed[:,None,None]\n",
    "        seds[k] = sed\n",
    "    if show:\n",
    "        rgb = img2rgb(images)\n",
    "        if figsize is not None:\n",
    "            plt.figure(figsize=figsize)\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    return images, centers, seds, morphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the models\n",
    "\n",
    "We see that autograd is slower than direct gradients but in our experience the majority of the time is spent on convolutions and this minor difference in runtime isn't significant. However, the factor of greater than 100 slowdown with JAX is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct gradients\n",
      "CPU times: user 38.8 ms, sys: 185 µs, total: 39 ms\n",
      "Wall time: 9.35 ms\n",
      "\n",
      "autograd gradients\n",
      "CPU times: user 293 ms, sys: 4.61 ms, total: 298 ms\n",
      "Wall time: 74.8 ms\n",
      "CPU times: user 5.43 s, sys: 852 ms, total: 6.28 s\n",
      "Wall time: 4.55 s\n",
      "\n",
      "JIT gradients\n",
      "CPU times: user 477 ms, sys: 8.87 ms, total: 486 ms\n",
      "Wall time: 470 ms\n",
      "\n",
      "\n",
      "These should all be approximately (0, 0)\n",
      "1.4551915228366852e-11 0.0\n",
      "0.00390625 0.0625\n",
      "0.00390625 0.0625\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "K = 300\n",
    "shape = (3, 300, 300)\n",
    "images, centers, seds, morphs = build_scene(shape, K)\n",
    "\n",
    "# Convert the data into autograd arrays\n",
    "images_autograd = anp.asarray(images)\n",
    "centers_autograd = anp.asarray(centers)\n",
    "seds_autograd = anp.asarray(seds)\n",
    "morphs_autograd = anp.asarray(morphs)\n",
    "\n",
    "# Convert the data into jax arrays\n",
    "images_jax = jnp.asarray(images)\n",
    "centers_jax = jnp.asarray(centers)\n",
    "seds_jax = jnp.asarray(seds)\n",
    "morphs_jax = jnp.asarray(morphs)\n",
    "\n",
    "print(\"Direct gradients\")\n",
    "%time g_direct = calculate_gradients_direct(images, centers, seds/2, morphs)\n",
    "\n",
    "print(\"\\nautograd gradients\")\n",
    "%time g_auto = calculate_gradients_autograd(images_autograd, centers_autograd, seds_autograd/2, morphs_autograd)\n",
    "\n",
    "#print(\"\\nJAX gradients\")\n",
    "%time g_jax = calculate_gradients_jax(images_jax, centers_jax, seds_jax/2, morphs_jax)\n",
    "\n",
    "print(\"\\nJIT gradients\")\n",
    "%time g_jax = calculate_gradients_jit(images_jax, centers_jax, seds_jax/2, morphs_jax)\n",
    "\n",
    "# Check that the results are the same\n",
    "print(\"\\n\\nThese should all be approximately (0, 0)\")\n",
    "print(np.max(np.abs(g_auto[0]-g_direct[0])), np.max(np.abs(g_auto[1]-g_direct[1])))\n",
    "print(np.max(np.abs(g_direct[0]-g_jax[0])), np.max(np.abs(g_direct[1]-g_jax[1])))\n",
    "print(np.max(np.abs(g_direct[0]-g_jit[0])), np.max(np.abs(g_direct[1]-g_jit[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating the issue\n",
    "\n",
    "We further isolate the issue by executing just the `get_model` functions, which just inserts the `(3,3,3)` model into the full `(3, 300, 300)` image and note the significant difference in runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.63 ms, sys: 1.21 ms, total: 5.84 ms\n",
      "Wall time: 5.37 ms\n",
      "CPU times: user 5.04 ms, sys: 570 µs, total: 5.61 ms\n",
      "Wall time: 5.04 ms\n",
      "CPU times: user 3.44 s, sys: 683 ms, total: 4.13 s\n",
      "Wall time: 3.11 s\n",
      "CPU times: user 128 ms, sys: 31.2 ms, total: 159 ms\n",
      "Wall time: 157 ms\n",
      "\n",
      "\n",
      "These should all be approximately (0, 0)\n",
      "0.0\n",
      "0.00024414062\n",
      "0.00024414062\n"
     ]
    }
   ],
   "source": [
    "%time m_direct = get_full_model(seds, morphs, centers, images.shape)[0]\n",
    "%time m_autograd = get_full_model_autograd(seds_autograd, morphs_autograd, centers_autograd, images.shape)[0]\n",
    "%time m_jax = get_full_model_jax(seds_jax, morphs_jax, centers_jax, images.shape)[0]\n",
    "%time m_jit = get_full_model_jit(seds_jax, morphs_jax, centers_jax, images.shape)\n",
    "\n",
    "print(\"\\n\\nThese should all be approximately (0, 0)\")\n",
    "print(np.max(m_direct-m_autograd))\n",
    "print(np.max(m_direct-m_jax))\n",
    "print(np.max(m_direct-m_jit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
